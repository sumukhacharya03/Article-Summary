{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Required Libraries and fixing the Data Path**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "JSN6A2pK7TKS"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer, BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "\n",
    "DATA_PATH = '/content/article_highlights.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**BERT-based Text Classification Pipeline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sys2_K7S6rXW"
   },
   "outputs": [],
   "source": [
    "class ArticleDataProcessor:\n",
    "    def __init__(self, data_path, tokenizer, max_length=512):\n",
    "        self.data_path = data_path\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load data from CSV file\"\"\"\n",
    "        df = pd.read_csv(self.data_path)\n",
    "        return df\n",
    "\n",
    "    def preprocess_data(self, df):\n",
    "        \"\"\"Preprocess data for BERT fine-tuning\"\"\"\n",
    "\n",
    "        def create_label(row):\n",
    "            # Convert to string to handle potential NaN values\n",
    "            article = str(row['article']).lower()\n",
    "            highlight = str(row['highlights']).lower()\n",
    "            # Check if the first 50 chars of highlight appear in the article\n",
    "            if highlight[:50] in article:\n",
    "                return 1\n",
    "            return 0\n",
    "\n",
    "        # Apply the function to create labels\n",
    "        df['label'] = df.apply(create_label, axis=1)\n",
    "\n",
    "        # Split data into train and validation sets\n",
    "        train_df, val_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "        return train_df, val_df\n",
    "\n",
    "    def prepare_features(self, text):\n",
    "        \"\"\"Convert text to BERT input features\"\"\"\n",
    "        return self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "class ArticleDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=512, is_training=True):\n",
    "        self.data = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.is_training = is_training\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Convert to string to handle potential NaN values\n",
    "        article = str(self.data.iloc[idx]['article'])\n",
    "        highlight = str(self.data.iloc[idx]['highlights'])\n",
    "\n",
    "        # Combine article and highlight for classification\n",
    "        text = f\"Article: {article} Highlight: {highlight}\"\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        # Convert dictionary of tensors to flat tensors\n",
    "        input_ids = encoding['input_ids'].squeeze()\n",
    "        attention_mask = encoding['attention_mask'].squeeze()\n",
    "\n",
    "        if self.is_training:\n",
    "            label = torch.tensor(self.data.iloc[idx]['label'], dtype=torch.long)\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask,\n",
    "                'label': label\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'input_ids': input_ids,\n",
    "                'attention_mask': attention_mask\n",
    "            }\n",
    "\n",
    "class BERTModelManager:\n",
    "    def __init__(self, num_labels=2, model_name='bert-base-uncased'):\n",
    "        \"\"\"Initialize the BERT model for fine-tuning\"\"\"\n",
    "        self.model_name = model_name\n",
    "        self.num_labels = num_labels\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "        self.model = BertForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "\n",
    "    def get_tokenizer(self):\n",
    "        return self.tokenizer\n",
    "\n",
    "    def get_model(self):\n",
    "        return self.model\n",
    "\n",
    "    def save_model(self, output_dir):\n",
    "        \"\"\"Save the fine-tuned model and tokenizer\"\"\"\n",
    "        if not os.path.exists(output_dir):\n",
    "            os.makedirs(output_dir)\n",
    "\n",
    "        self.model.save_pretrained(output_dir)\n",
    "        self.tokenizer.save_pretrained(output_dir)\n",
    "        print(f\"Model saved to {output_dir}\")\n",
    "\n",
    "class BERTTrainer:\n",
    "    def __init__(self, model, tokenizer, train_dataloader, val_dataloader,\n",
    "                 device, epochs=3, learning_rate=2e-5):\n",
    "        self.model = model\n",
    "        self.tokenizer = tokenizer\n",
    "        self.train_dataloader = train_dataloader\n",
    "        self.val_dataloader = val_dataloader\n",
    "        self.device = device\n",
    "        self.epochs = epochs\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def train(self):\n",
    "        \"\"\"Train the BERT model\"\"\"\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Set up optimizer and scheduler\n",
    "        optimizer = AdamW(self.model.parameters(), lr=self.learning_rate)\n",
    "        total_steps = len(self.train_dataloader) * self.epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        for epoch in range(self.epochs):\n",
    "            print(f\"\\nEpoch {epoch+1}/{self.epochs}\")\n",
    "            self.model.train()\n",
    "            total_loss = 0\n",
    "\n",
    "            # Progress bar for training\n",
    "            progress_bar = tqdm(self.train_dataloader, desc=\"Training\")\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n",
    "\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'loss': f\"{loss.item():.4f}\"})\n",
    "\n",
    "            avg_train_loss = total_loss / len(self.train_dataloader)\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            # Validation after each epoch\n",
    "            val_loss, val_accuracy = self.evaluate()\n",
    "            print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluate the model on validation data\"\"\"\n",
    "        self.model.eval()\n",
    "        val_loss = 0\n",
    "        predictions = []\n",
    "        true_labels = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in tqdm(self.val_dataloader, desc=\"Evaluating\"):\n",
    "                input_ids = batch['input_ids'].to(self.device)\n",
    "                attention_mask = batch['attention_mask'].to(self.device)\n",
    "                labels = batch['label'].to(self.device)\n",
    "\n",
    "                outputs = self.model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                logits = outputs.logits\n",
    "                preds = torch.argmax(logits, dim=1).cpu().numpy()\n",
    "\n",
    "                predictions.extend(preds)\n",
    "                true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "        val_loss = val_loss / len(self.val_dataloader)\n",
    "        val_accuracy = accuracy_score(true_labels, predictions)\n",
    "\n",
    "        print(\"\\nClassification Report:\")\n",
    "        print(classification_report(true_labels, predictions))\n",
    "\n",
    "        return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training the Model and Saving the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tULiwBcG6rsc",
    "outputId": "9ed34995-f43e-4410-e129-97673fe58daa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training samples: 6540\n",
      "Validation samples: 1636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 818/818 [10:31<00:00,  1.30it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0077\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [00:50<00:00,  4.08it/s]\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         3\n",
      "           1       1.00      1.00      1.00      1633\n",
      "\n",
      "    accuracy                           1.00      1636\n",
      "   macro avg       0.50      0.50      0.50      1636\n",
      "weighted avg       1.00      1.00      1.00      1636\n",
      "\n",
      "Validation Loss: 0.0019\n",
      "Validation Accuracy: 0.9982\n",
      "\n",
      "Epoch 2/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 818/818 [10:30<00:00,  1.30it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [00:50<00:00,  4.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00      1633\n",
      "\n",
      "    accuracy                           1.00      1636\n",
      "   macro avg       1.00      1.00      1.00      1636\n",
      "weighted avg       1.00      1.00      1.00      1636\n",
      "\n",
      "Validation Loss: 0.0001\n",
      "Validation Accuracy: 1.0000\n",
      "\n",
      "Epoch 3/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 818/818 [10:30<00:00,  1.30it/s, loss=0.0000]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average training loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 205/205 [00:50<00:00,  4.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00         3\n",
      "           1       1.00      1.00      1.00      1633\n",
      "\n",
      "    accuracy                           1.00      1636\n",
      "   macro avg       1.00      1.00      1.00      1636\n",
      "weighted avg       1.00      1.00      1.00      1636\n",
      "\n",
      "Validation Loss: 0.0000\n",
      "Validation Accuracy: 1.0000\n",
      "Model saved to /content/drive/MyDrive/tdl_orange_problem/fine_tuned_bert\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def main():\n",
    "    # Setup device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Setup BERT model and tokenizer\n",
    "    model_manager = BERTModelManager()\n",
    "    tokenizer = model_manager.get_tokenizer()\n",
    "    model = model_manager.get_model()\n",
    "\n",
    "    # Load and preprocess data\n",
    "    data_processor = ArticleDataProcessor(DATA_PATH, tokenizer)\n",
    "    df = data_processor.load_data()\n",
    "    train_df, val_df = data_processor.preprocess_data(df)\n",
    "\n",
    "    print(f\"Training samples: {len(train_df)}\")\n",
    "    print(f\"Validation samples: {len(val_df)}\")\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = ArticleDataset(train_df, tokenizer)\n",
    "    val_dataset = ArticleDataset(val_df, tokenizer)\n",
    "\n",
    "    # Create dataloaders\n",
    "    batch_size = 8\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=batch_size)\n",
    "\n",
    "    # Train the model\n",
    "    trainer = BERTTrainer(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        train_dataloader=train_dataloader,\n",
    "        val_dataloader=val_dataloader,\n",
    "        device=device,\n",
    "        epochs=3\n",
    "    )\n",
    "\n",
    "    fine_tuned_model = trainer.train()\n",
    "\n",
    "    # Save the model\n",
    "    a = '/content/drive/MyDrive/tdl_orange_problem'\n",
    "    output_dir = os.path.join(a, 'fine_tuned_bert')\n",
    "    model_manager.save_model(output_dir)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing Required Libraries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "id": "k4sAv0cBXMYz"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "from transformers import BertTokenizer, BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Saves the File Path where the Bert Model is saved**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aPdZJFW5XOWp"
   },
   "outputs": [],
   "source": [
    "bert_model_path = '/content/drive/MyDrive/tdl_orange_problem/fine_tuned_bert'  # Update this path as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Load the Tokenizer and the Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WBF17QfsXTr3"
   },
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained(bert_model_path)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To make sure to use the same device as used during training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fAajPAteXWGH",
    "outputId": "cf05798e-a9f0-4e65-c061-7fd8dd6cab8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BertForSequenceClassification(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Extractive Summarization Function using BERT**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_fAc7ZwXaoG"
   },
   "outputs": [],
   "source": [
    "def simple_sentence_tokenize(text):\n",
    "    \"\"\"\n",
    "    A simple function to split text into sentences without using NLTK\n",
    "    \"\"\"\n",
    "    # Split on period, exclamation mark, or question mark followed by space\n",
    "    sentences = re.split(r'(?<=[.!?])\\s+', text)\n",
    "    # Filter out empty sentences\n",
    "    return [s for s in sentences if s.strip()]\n",
    "\n",
    "def extractive_summarize(article, num_sentences=3):\n",
    "    \"\"\"\n",
    "    Use BERT to perform extractive summarization by ranking sentences.\n",
    "\n",
    "    Args:\n",
    "        article (str): The input article text\n",
    "        num_sentences (int): Number of sentences to include in the summary\n",
    "\n",
    "    Returns:\n",
    "        str: The extractive summary\n",
    "    \"\"\"\n",
    "    # Split the article into sentences using our custom function\n",
    "    sentences = simple_sentence_tokenize(article)\n",
    "\n",
    "    if len(sentences) <= num_sentences:\n",
    "        return article\n",
    "\n",
    "    # Store sentence scores\n",
    "    scores = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence\n",
    "        inputs = tokenizer(\n",
    "            sentence,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            max_length=512,\n",
    "            return_tensors='pt'\n",
    "        ).to(device)\n",
    "\n",
    "        # Get the prediction logits\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "            # Use the positive class logit as the sentence importance score\n",
    "            score = outputs.logits[0][1].item()\n",
    "            scores.append(score)\n",
    "\n",
    "    # Select top sentences\n",
    "    top_indices = np.argsort(scores)[-num_sentences:]\n",
    "    top_indices = sorted(top_indices)\n",
    "\n",
    "    # Form the summary\n",
    "    summary = ' '.join([sentences[i] for i in top_indices])\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Generating Summary**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J49b-Nt9XHdN",
    "outputId": "3a92cdca-1075-4e37-b2b7-a7e2dfa9f3df"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Article:\n",
      "\n",
      "Liverpool secured a commanding 4-1 victory over Luton Town in the Premier League match. The Reds initially fell behind in the opening minutes but quickly recovered with goals from Van Dijk, Gakpo, and a brace from Salah. The win helps Liverpool maintain their position at the top of the Premier League table. Manager JÃ¼rgen Klopp praised the team's resilience and attacking prowess, particularly highlighting the performance of their Egyptian forward. Luton Town, despite the defeat, showed moments of quality but ultimately couldn't match Liverpool's class throughout the 90 minutes.\n",
      "\n",
      "\n",
      "--------------------------------------------------\n",
      "\n",
      "BERT Extractive Summary:\n",
      "\n",
      "Liverpool secured a commanding 4-1 victory over Luton Town in the Premier League match. The Reds initially fell behind in the opening minutes but quickly recovered with goals from Van Dijk, Gakpo, and a brace from Salah. The win helps Liverpool maintain their position at the top of the Premier League table.\n"
     ]
    }
   ],
   "source": [
    "# Example article\n",
    "article_example = \"\"\"\n",
    "Liverpool secured a commanding 4-1 victory over Luton Town in the Premier League match. The Reds initially fell behind in the opening minutes but quickly recovered with goals from Van Dijk, Gakpo, and a brace from Salah. The win helps Liverpool maintain their position at the top of the Premier League table. Manager JÃ¼rgen Klopp praised the team's resilience and attacking prowess, particularly highlighting the performance of their Egyptian forward. Luton Town, despite the defeat, showed moments of quality but ultimately couldn't match Liverpool's class throughout the 90 minutes.\n",
    "\"\"\"\n",
    "\n",
    "# Generate and print summary\n",
    "print(\"Original Article:\")\n",
    "print(article_example)\n",
    "print(\"\\n\" + \"-\"*50 + \"\\n\")\n",
    "\n",
    "print(\"BERT Extractive Summary:\")\n",
    "summary = extractive_summarize(article_example)\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
